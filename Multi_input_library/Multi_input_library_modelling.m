
%% Creating data splits (from scratch)
ohes = [];

for i = 1:size(eu_ordered_ass_split, 2)
    a = categorical(eu_ordered_ass_split(:, i));
    a = onehotencode(a, 2);
    ohes(1:size(a, 1), end+1:end+size(a, 2)) = a;
end

ohe_news = ohes(ordered_eu_exp(:, 1) > 0 & ordered_eu_exp(:, 2) > 0 & ordered_eu_exp(:, 3) > 0 & ordered_eu_exp(:, 4) > 0, :);
out_channel = 1;

outputs = log10(ordered_eu_exp(ordered_eu_exp(:, 1) > 0 & ordered_eu_exp(:, 2) > 0 & ordered_eu_exp(:, 3) > 0 & ordered_eu_exp(:, 4) > 0, 4*(out_channel - 1) + 1:4*out_channel));
output_bcs = sum(ordered_eu_exp(ordered_eu_exp(:, 1) > 0 & ordered_eu_exp(:, 2) > 0 & ordered_eu_exp(:, 3) > 0 & ordered_eu_exp(:, 4) > 0, 13:16), 2);

%Make test set (high quality)
outputs_test = outputs(output_bcs > 50, :);
ohe_test = ohe_news(output_bcs > 50, :);

%Compute the ones left
ohe_left = ohe_news; ohe_left(output_bcs > 50, :) = [];
outputs_left = outputs; outputs_left(output_bcs > 50, :) = [];

%Add 50% of the high quality test set back into the training set (not
%needed)
r = randsample(size(ohe_test, 1), floor(0.50*size(ohe_test, 1)), false);
ohe_left(end+1:end+length(r), :) = ohe_test(r, :);
outputs_left(end+1:end+length(r), :) = outputs_test(r, :);
outputs_test(r, :) = []; ohe_test(r, :) = [];
bcs_test = output_bcs(output_bcs > 50); bcs_test(r, :) = [];
bc_space = linspace(min(x), max(x), 20);

%Randomly split 10% of the remaining set for validation
r = randsample(size(ohe_left, 1), floor(0.1*size(ohe_left, 1)), false);

ohe_train = ohe_left; ohe_train(r, :) = [];
outputs_train = outputs_left; outputs_train(r, :) = [];

ohe_val = ohe_left(r, :); outputs_val = outputs_left(r, :);


%% Training (from saved train/test/val splits)

%Load variables
load('Trained_MLP_Model_and_Variables.mat')

%Define layers
layers = [
    featureInputLayer(41)
    fullyConnectedLayer(160)
    tanhLayer
    fullyConnectedLayer(80)
    tanhLayer
    fullyConnectedLayer(40)
    tanhLayer
    fullyConnectedLayer(20)
    tanhLayer
    fullyConnectedLayer(10)
    tanhLayer
    fullyConnectedLayer(4)
    reluLayer];

%Create network
net2 = dlnetwork(layers);

%Set options
options = trainingOptions("sgdm", ...
    MaxEpochs=200, ...
    Verbose=false, ...
    LearnRateSchedule="piecewise", ...
    LearnRateDropFactor=0.95, ...
    LearnRateDropPeriod=20, ...
    Plots="training-progress", ...
    Shuffle="every-epoch", ...
    Metrics="rmse", ...
    ValidationFrequency=10, ...
    InitialLearnRate=5e-2, ...
    ValidationData=table(ohe_val, outputs_val), ...
    ValidationPatience=300, ...
    OutputNetwork="best-validation", ...
    MiniBatchSize=256, ...
    ExecutionEnvironment = "gpu");

%Train Network
trainednet2 = trainnet(table(ohe_train, outputs_train), net2, "mse", options);

%Compute performance on test set
pred = minibatchpredict(trainednet2, ohe_test);

%% Hyper parameter optimization

%Vary learning rates, layers, and amount of data used for training
lrs = 10.^linspace(-3, -1, 5);
ls = {layers([1:3 12:13]), layers([1:5 12:13]), layers([1:7 12:13]), layers([1:9 12:13]), layers};
dats = floor(linspace(1000, size(ohe_train, 1), 5));

r2s_train = cell(length(lrs), length(dats), length(ls));
r2s_test = cell(length(lrs), length(dats), length(ls));
r2s_val = cell(length(lrs), length(dats), length(ls));

for i = 1:length(lrs)
    for j = 1:length(dats)
        disp(['i = ', num2str(i), ', j = ', num2str(j)])
        r = randsample(size(ohe_train, 1), dats(j), false);
        t = ohe_train(r, :);
        exp = outputs_train(r, :);
        for k = 1:length(ls)
            l = ls{k}; net = dlnetwork(l);

            options = trainingOptions("sgdm", ...
            MaxEpochs=50, ...
            Verbose=false, ...
            LearnRateSchedule="piecewise", ...
            LearnRateDropFactor=0.95, ...
            LearnRateDropPeriod=20, ...
            Shuffle="every-epoch", ...
            Metrics="rmse", ...
            ValidationFrequency=5, ...
            InitialLearnRate=lrs(i), ...
            ValidationData=table(ohe_val, outputs_val), ...
            ValidationPatience=100, ...
            OutputNetwork="best-validation", ...
            MiniBatchSize=256, ...
            ExecutionEnvironment = "gpu");
            
            trainednet = trainnet(table(t, exp), net2, "mse", options);
            pred = minibatchpredict(trainednet, ohe_test);
            pt = minibatchpredict(trainednet, t);
            pv = minibatchpredict(trainednet, ohe_val);
            r2s_train(i, j, k) = {[diag(corr(pt, exp))].^2};
            r2s_val(i, j, k) = {[diag(corr(pv, outputs_val))].^2};
            r2s_test(i, j, k) = {[diag(corr(pred, outputs_test))].^2};
        end
    end
end

clear x; clear y; clear z;
for i = 1:size(r2s_val, 1)
    for j = 1:size(r2s_val, 3)
        for k = 1:size(r2s_val, 2)
            x1 = r2s_val{i, k, j};
            x(i, j, k) = x1(1);
            y(i, j, k) = x1(2);
            z(i, j, k) = x1(3);
            w(i, j, k) = x1(4);
        end
    end
end

% Now optimize solver (sgdm vs adam) & dropout

ms = 0:0.1:0.9;
r2s_valm = zeros(size(ms));

for i = 1:length(ms)
    disp(i)
    options = trainingOptions("sgdm", ...
        Momentum=ms(i), ...
        MaxEpochs=50, ...
        Verbose=false, ...
        LearnRateSchedule="piecewise", ...
        LearnRateDropFactor=0.95, ...
        LearnRateDropPeriod=20, ...
        Shuffle="every-epoch", ...
        Metrics="rmse", ...
        ValidationFrequency=10, ...
        InitialLearnRate=5e-2, ...
        ValidationData=table(ohe_val, outputs_val), ...
        ValidationPatience=100, ...
        OutputNetwork="best-validation", ...
        MiniBatchSize=256, ...
        ExecutionEnvironment = "gpu");
    
    trainednet = trainnet(table(ohe_train, outputs_train), net2, "mse", options);
    pv = minibatchpredict(trainednet, ohe_val);
    x = diag(corr(pv, outputs_val)).^2;
    r2s_valm(i) = x(1);
end

options = trainingOptions("lbfgs", ...
        Verbose=false, ...
        Metrics="rmse", ...
        ValidationFrequency=10, ...
        ValidationData=table(ohe_val, outputs_val), ...
        ValidationPatience=100, ...
        OutputNetwork="best-validation", ...
        ExecutionEnvironment = "gpu");
    
trainednet = trainnet(table(ohe_train, outputs_train), net2, "mse", options);
pv = minibatchpredict(trainednet, ohe_val);
x = diag(corr(pv, outputs_val)).^2;

options = trainingOptions("sgdm", ...
    Momentum=ms(i), ...
    MaxEpochs=50, ...
    Verbose=false, ...
    LearnRateSchedule="piecewise", ...
    LearnRateDropFactor=0.95, ...
    LearnRateDropPeriod=20, ...
    Shuffle="every-epoch", ...
    Metrics="rmse", ...
    Plots="training-progress", ...
    ValidationFrequency=10, ...
    InitialLearnRate=5e-2, ...
    ValidationData=table(ohe_val, outputs_val), ...
    ValidationPatience=100, ...
    OutputNetwork="best-validation", ...
    MiniBatchSize=256, ...
    ExecutionEnvironment = "gpu");

[trainednet, tr] = trainnet(table(ohe_train, outputs_train), net2, "mse", options);
